{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model: Autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "#### [1. Load and Preprocess Data](#load)\n",
    "#### [2. Develope N-gram Based Language Model](#ngram)\n",
    "#### [3. Perplexity](#prep)\n",
    "#### [4. Build an Autocomplete System](#auto)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('.')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en_US.twitter.txt\",encoding=\"utf8\") as f:\n",
    "    data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data type: <class 'str'>\n",
      " Number of letters: 3335477\n",
      " First 200 letters: How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\n",
      "When you meet someone special... you'll know. Your heart will beat more rapidly and you'll\n"
     ]
    }
   ],
   "source": [
    "print(f\" Data type: {type(data)}\")\n",
    "print(f\" Number of letters: {len(data)}\")\n",
    "print(f\" First 200 letters: {data[0:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Data\n",
    "- Split data using \"\\n\"\n",
    "- Split each sentence into words\n",
    "- Split data into train and test dataset\n",
    "- Find word counts\n",
    "- Replace words that appears less than N times by <unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Spliting data by linebreak\n",
    "    Args:\n",
    "        data: str\n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split(\"\\n\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens(words)\n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    tokenize_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenize = nltk.word_tokenize(sentence)\n",
    "        tokenize_sentences.append(tokenize)\n",
    "    return tokenize_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = split_to_sentences(data)\n",
    "tokenized_data = tokenize_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(87)\n",
    "random.shuffle(tokenized_data)\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total letters: 47961,Train data letters: 38368, Test data letters: 9593'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Total letters: {len(tokenized_data)},Train data letters: {len(train_data)}, Test data letters: {len(test_data)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Words Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenize_sentences):\n",
    "    \"\"\"\n",
    "    Count number of words in tokenized sentences\n",
    "    Args:\n",
    "        tokenize_sentences: List of lists of words\n",
    "    Returns:\n",
    "        A dictionary that maps word to the freq\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "\n",
    "    for sentence in tokenize_sentences:\n",
    "        for word in sentence:\n",
    "            word_counts[word] = word_counts.get(word, 0) +1\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Vocabulary(OOV) Words\n",
    "\n",
    "- Create a list of the most frequent words in the training set, called the closed vocabulary .\n",
    "- Convert all the other words that are not part of the closed vocabulary to the token 'unk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_freq(tokenize_sentences, count_threshold):\n",
    "\n",
    "    \"\"\"\n",
    "    Looking for words with N appearance or more.\n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    Returns:\n",
    "        List of Words that appear N times or more\n",
    "    \"\"\"\n",
    "\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenize_sentences)\n",
    "\n",
    "    for word,cnt in word_counts.items():\n",
    "        if cnt>= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words(tokenize_sentences, vocabulary, unknown_token = \"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the vocabulary with unknown_token.\n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing OOV words\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentece in tokenize_sentences:\n",
    "        replaced_sentence = []\n",
    "        for word in sentece:\n",
    "            if word in vocabulary:\n",
    "                replaced_sentence.append(word)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Pre-process data\n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    Returns:\n",
    "        Tuple of (\n",
    "        - pre-process training data\n",
    "        - pre-process test data\n",
    "        - Vocabulary that appears n times or more in training data\n",
    "        )\n",
    "    \"\"\"\n",
    "    vocabulary = get_words_with_nplus_freq(train_data, count_threshold)\n",
    "    train_data_replaced = replace_oov_words(train_data, vocabulary)\n",
    "    test_data_replaced = replace_oov_words(test_data, vocabulary)\n",
    "    return train_data_replaced, test_data_replaced, vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develope N-gram Based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram(data, n, start_token = \"<s>\", end_token = \"<e>\"):\n",
    "    \"\"\"\n",
    "    Count n-grams in data\n",
    "    Args:\n",
    "        data: List of list of words.\n",
    "        n: number of words in a sequence.\n",
    "    Returns:\n",
    "        A dictionary woth tuple of n-words as keys and freq as values.\n",
    "    \"\"\"\n",
    "    n_gram_dict = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "\n",
    "        sentence = tuple(sentence)\n",
    "        if n==1:\n",
    "            m = len(sentence)\n",
    "        else:\n",
    "            m = len(sentence)-1\n",
    "        \n",
    "        for i in range(m):\n",
    "            ngram = sentence[i:i+n]\n",
    "            n_gram_dict[ngram] = n_gram_dict.get(ngram, 0) +1\n",
    "    return n_gram_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Probability Using K-smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1):\n",
    "    \"\"\"\n",
    "    Estimate the probability of next word using ngram and k-smoothing\n",
    "    Args:\n",
    "        word: next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of words in the vocabulary\n",
    "        k: positive constant, smoothing parameter\n",
    "    Returns:\n",
    "        Probability of word\n",
    "    \"\"\"\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    numerator = n_plus1_gram_count + k \n",
    "    probability = numerator/denominator\n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function below calculates probabilities for all possible words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    vocabulary = vocabulary + [\"<unk>\", \"<e>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, \n",
    "        n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count and Probability Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    n_grams = []\n",
    "    for nplus1_gram in n_plus1_gram_counts.keys():\n",
    "        ngram = nplus1_gram[:-1]\n",
    "        n_grams.append(ngram)\n",
    "    n_grams = list(set(n_grams))\n",
    "\n",
    "    row_index = {ngram:i for i,ngram in enumerate(n_grams)}\n",
    "    col_index = {word:j for j,word in enumerate(vocabulary)}\n",
    "\n",
    "    nrows = len(n_grams)\n",
    "    ncols = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrows, ncols))\n",
    "\n",
    "    for nplus1_gram, count in n_plus1_gram_counts.items():\n",
    "        ngram = nplus1_gram[:-1]\n",
    "        word = nplus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[ngram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i,j] = count\n",
    "    count_matrix = pd.DataFrame(count_matrix, index = n_grams, columns = vocabulary)\n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis = 1), axis = 0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for list of sentences\n",
    "    Args:\n",
    "        sentence: List of strings\n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: number of unique words in the vocabulary\n",
    "        k: Positive smoothing constant\n",
    "    Returns:\n",
    "        perplexity score\n",
    "    \"\"\"\n",
    "    #length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    sentence = [\"<s>\"] + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    N = len(sentence)\n",
    "    product = 1\n",
    "\n",
    "    for t in range(n, N):\n",
    "        ngram = sentence[t-n:t]\n",
    "        word = sentence[t]\n",
    "        probability = estimate_probability(word,ngram, n_gram_counts, nplus1_gram_counts, len(unique_words), k=1)\n",
    "        product *= 1/probability\n",
    "    \n",
    "    perplexity = product**(1/float(N))\n",
    "    return perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Auto-complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \"\"\"\n",
    "    Suggest the next word\n",
    "    Args:\n",
    "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
    "        n_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary: List of words\n",
    "        k: positive constant, smoothing parameter\n",
    "        start_with: If not None, specifies the first few letters of the next word\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of \n",
    "          - string of the most likely next word\n",
    "          - corresponding probability\n",
    "    \"\"\"\n",
    "    #length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestions = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items():\n",
    "        if start_with != None:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        if prob> max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_ngram(sentences, 1)\n",
    "bigram_counts = count_ngram(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Multiple Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "        model_counts = len(n_gram_counts_list)\n",
    "        suggestions = []\n",
    "        for i in range(model_counts-1):\n",
    "            ngram_counts = n_gram_counts_list[i]\n",
    "            n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "            suggestion = suggest_a_word(previous_tokens, ngram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "            suggestions.append(suggestion)\n",
    "        return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('a', 0.1111111111111111),\n",
       " ('a', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'],\n",
    "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_ngram(sentences, 1)\n",
    "bigram_counts = count_ngram(sentences, 2)\n",
    "trigram_counts = count_ngram(sentences, 3)\n",
    "quadgram_counts = count_ngram(sentences, 4)\n",
    "qintgram_counts = count_ngram(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_ngram(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'am', 'to'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be', 0.02766367370678687),\n",
       " ('have', 0.00013485267345425124),\n",
       " ('have', 0.00013488905375328792),\n",
       " ('i', 6.745362563237774e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"am\", \"to\"]\n",
    "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('to', 0.014050206069689023),\n",
       " ('to', 0.004697320542507443),\n",
       " ('to', 0.0009423167530457024),\n",
       " ('to', 0.00040439441935701285)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
    "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 0.023424142254644932),\n",
       " ('you', 0.0035589578297072254),\n",
       " ('you', 0.00013489815189531904),\n",
       " ('i', 6.745362563237774e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
    "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
